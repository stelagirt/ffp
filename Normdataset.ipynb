{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import sklearn.metrics\n",
    "from numpy import unravel_index\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalized_values(y,dfmax, dfmin):\n",
    "    a = (y- dfmin) / (dfmax - dfmin)\n",
    "    return(a)\n",
    "\n",
    "def apply_norm(normdf, unnormdf, col, dfmax, dfmin, dfmean, dfstd, norm_type):\n",
    "    normdf[col] = unnormdf.apply(lambda x: normalized_values(x[col], dfmax, dfmin, dfmean, dfstd, norm_type), axis=1)\n",
    "\n",
    "def dataset_sanity_check(df):\n",
    "    for c in [cl for cl in df.columns if 'bin' not in cl]:\n",
    "        print('column %s - max: %s, min : %s, mean: %s, std: %s'%(c, df[c].max(), df[c].min(), df[c].mean(), df[c].std()))\n",
    "\n",
    "def normalize_dataset(df, d):\n",
    "    X = DataFrame()\n",
    "#    if aggrfile and os.path.exists(aggrfile):\n",
    "#        with open(aggrfile) as aggrf:\n",
    "#            aggrs = json.loads(aggrf.read())\n",
    "    for c in df.columns:\n",
    "        if c in d.keys():\n",
    "            print(c)\n",
    "            dfcfloat = df[c].astype('float64')\n",
    "            #print(\"Normalize column:%s\" % c)\n",
    "            dfmax = d[c]['max'] if 'max' in d[c] else None\n",
    "            dfmin = d[c]['min'] if 'min' in d[c] else None\n",
    "            #print(d[c]['min'],'-',d[c]['max'])\n",
    "            #dfmean = d[c]['mean'] if 'mean' in aggrs[c] else None\n",
    "            #dfstd = d[c]['std'] if 'std' in aggrs[c] else None\n",
    "            X[c] = dfcfloat.apply(lambda x: normalized_values(x, dfmax, dfmin))#, axis=1)\n",
    "            #dataset_sanity_check(X[[c]])\n",
    "        else:\n",
    "            X[c] = df[c]\n",
    "        X[c] = X[c].round(3)\n",
    "    return(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = {'aspect': {'max': 359.99800000000016, 'min': -1.0},\n",
    " 'curvature': {'max': 0.73661599999999994, 'min': -0.76867799999999997},\n",
    " 'dem': {'max': 2806.1399999999999, 'min': -6.04},\n",
    " 'dom_vel': {'max': 19.337629295696285, 'min': 0.49528943306704032},\n",
    " 'evi': {'max': 0.99250000000000005, 'min': -0.19869999999999999},\n",
    " 'lst_day': {'max': 342.77999999999997, 'min': 251.78},\n",
    " 'lst_night': {'max': 322.45999999999998, 'min': 254.47999572753903},\n",
    " 'max_dew_temp': {'max': 300.5537109375, 'min': 266.383056640625},\n",
    " 'max_temp': {'max': 317.44775390625, 'min': 273.447509765625},\n",
    " 'mean_dew_temp': {'max': 298.55347696940106, 'min': 263.98918660481769},\n",
    " 'mean_temp': {'max': 309.74698893229169, 'min': 270.31141153971362},\n",
    " 'min_dew_temp': {'max': 297.611083984375, 'min': 258.5595703125},\n",
    " 'min_temp': {'max': 303.00634765625, 'min': 262.879150390625},\n",
    " 'ndvi_new': {'max': 0.99939999999999996, 'min': -0.20000000000000001},\n",
    " 'rain_7days': {'max': 3.5451469016037436, 'min': 6.8902961061212417e-06},\n",
    " 'res_max': {'max': 19.337629295696285, 'min': 2.7861490154313331},\n",
    " 'slope': {'max': 45.0045, 'min': 0.0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df = pd.read_csv('/work2/pa21/sgirtsou/production/2010/05/may_2010_dummies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict = {}\n",
    "for folder, subfolders, files in os.walk('/work2/pa21/sgirtsou/production', topdown=True):\n",
    "    for file in files:\n",
    "        if file.endswith('dummies.csv'):\n",
    "            if file.split('_')[1] in dict:\n",
    "                dict[file.split('_')[1]].append(os.path.join(os.path.abspath(folder), file))\n",
    "            else:\n",
    "                dict[file.split('_')[1]]=[]\n",
    "                dict[file.split('_')[1]].append(os.path.join(os.path.abspath(folder), file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2010': ['/work2/pa21/sgirtsou/production/2010/05/may_2010_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2010/08/august_2010_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2010/09/september_2010_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2010/04/april_2010_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2010/07/july_2010_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2010/06/june_2010_dummies.csv'],\n",
       " '2011': ['/work2/pa21/sgirtsou/production/2011/05/may_2011_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2011/08/august_2011_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2011/09/september_2011_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2011/04/april_2011_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2011/07/july_2011_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2011/06/june_2011_dummies.csv'],\n",
       " '2012': ['/work2/pa21/sgirtsou/production/2012/05/may_2012_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2012/08/august_2012_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2012/09/september_2012_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2012/04/april_2012_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2012/07/july_2012_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2012/06/june_2012_dummies.csv'],\n",
       " '2013': ['/work2/pa21/sgirtsou/production/2013/05/may_2013_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2013/08/august_2013_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2013/09/september_2013_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2013/04/april_2013_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2013/07/july_2013_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2013/06/june_2013_dummies.csv'],\n",
       " '2014': ['/work2/pa21/sgirtsou/production/2014/05/may_2014_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2014/08/august_2014_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2014/09/september_2014_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2014/04/april_2014_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2014/07/july_2014_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2014/06/june_2014_dummies.csv'],\n",
       " '2015': ['/work2/pa21/sgirtsou/production/2015/05/may_2015_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2015/08/august_2015_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2015/09/september_2015_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2015/04/april_2015_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2015/07/july_2015_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2015/06/june_2015_dummies.csv'],\n",
       " '2016': ['/work2/pa21/sgirtsou/production/2016/05/may_2016_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2016/08/august_2016_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2016/09/september_2016_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2016/04/april_2016_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2016/07/july_2016_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2016/06/june_2016_dummies.csv'],\n",
       " '2017': ['/work2/pa21/sgirtsou/production/2017/05/may_2017_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2017/08/august_2017_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2017/09/september_2017_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2017/04/april_2017_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2017/07/july_2017_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2017/06/june_2017_dummies.csv'],\n",
       " '2018': ['/work2/pa21/sgirtsou/production/2018/05/may_2018_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2018/08/august_2018_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2018/09/september_2018_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2018/04/april_2018_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2018/07/july_2018_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2018/06/june_2018_dummies.csv'],\n",
       " '2019': ['/work2/pa21/sgirtsou/production/2019/05/may_2019_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2019/08/august_2019_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2019/09/september_2019_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2019/04/april_2019_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2019/07/july_2019_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/2019/06/june_2019_dummies.csv'],\n",
       " 'dataset': ['/work2/pa21/sgirtsou/production/datasets/randomnofire/training_dataset_dew_lst_dummies.csv'],\n",
       " 'dummies.csv': ['/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/2014_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/2013_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/2017_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/2016_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/2012_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/2018_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/2011_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/2010_dummies.csv',\n",
       "  '/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/2015_dummies.csv'],\n",
       " 'training': ['/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/old_training_dataset_dew_lst_dummies.csv']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict={'2015': ['/work2/pa21/sgirtsou/production/2015/05/may_2015_dummies.csv',\n",
    "  '/work2/pa21/sgirtsou/production/2015/08/august_2015_dummies.csv',\n",
    "  '/work2/pa21/sgirtsou/production/2015/09/september_2015_dummies.csv',\n",
    "  '/work2/pa21/sgirtsou/production/2015/04/april_2015_dummies.csv',\n",
    "  '/work2/pa21/sgirtsou/production/2015/07/july_2015_dummies.csv',\n",
    "  '/work2/pa21/sgirtsou/production/2015/06/june_2015_dummies.csv']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns = ['max_temp', 'min_temp', 'mean_temp',\n",
    "       'res_max', 'dom_vel', 'rain_7days', 'dem', 'slope', 'curvature',\n",
    "       'aspect', 'ndvi_new', 'evi', 'lst_day', 'lst_night', 'max_dew_temp',\n",
    "       'mean_dew_temp', 'min_dew_temp', 'fire', 'dir_max_1', 'dir_max_2',\n",
    "       'dir_max_3', 'dir_max_4', 'dir_max_5', 'dir_max_6', 'dir_max_7',\n",
    "       'dir_max_8', 'dom_dir_1', 'dom_dir_2', 'dom_dir_3', 'dom_dir_4',\n",
    "       'dom_dir_5', 'dom_dir_6', 'dom_dir_7', 'dom_dir_8', 'corine_111',\n",
    "       'corine_112', 'corine_121', 'corine_122', 'corine_123', 'corine_124',\n",
    "       'corine_131', 'corine_132', 'corine_133', 'corine_141', 'corine_142',\n",
    "       'corine_211', 'corine_212', 'corine_213', 'corine_221', 'corine_222',\n",
    "       'corine_223', 'corine_231', 'corine_241', 'corine_242', 'corine_243',\n",
    "       'corine_244', 'corine_311', 'corine_312', 'corine_313', 'corine_321',\n",
    "       'corine_322', 'corine_323', 'corine_324', 'corine_331', 'corine_332',\n",
    "       'corine_333', 'corine_334', 'corine_411', 'corine_412', 'corine_421',\n",
    "       'corine_422', 'corine_511', 'corine_512', 'corine_521', 'wkd_0',\n",
    "       'wkd_1', 'wkd_2', 'wkd_3', 'wkd_4', 'wkd_5', 'wkd_6', 'month_5',\n",
    "       'month_4', 'month_6', 'month_7', 'month_8', 'month_9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(fires,nofires):\n",
    "    #fires = df[df.fire == 1][columns]\n",
    "    n = fires.max_temp.count()\n",
    "#   print(n,' fires found')\n",
    "    #nofires = df[df['fire'] == 0][columns]\n",
    "    #if n == 0:\n",
    "    #    return 0,0\n",
    "    #else:\n",
    "    print('Finding similarity....')\n",
    "    similarity = sklearn.metrics.pairwise.cosine_similarity(nofires[columns], Y=fires[columns], dense_output=True)\n",
    "    return n,similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_dataset(fires,nofires,df1):\n",
    "    f_list=[]\n",
    "    #n,similarity = fires.apply(lambda x:cosine_similarity(pd.DataFrame(x),nofires,similarity),axis=1)\n",
    "    n, similarity = cosine_similarity(fires,nofires)\n",
    "    print('Creating similarity dataframe')\n",
    "    sim_df = pd.DataFrame(data = similarity,index=nofires.index,columns=fires.index)\n",
    "    print('similarity dataframe ok')\n",
    "    process_sim = sim_df\n",
    "    ind = list(process_sim.nlargest(n*20, sim_df.columns).index)\n",
    "    print('Found',n*20,'largest similarities')\n",
    "    f_list.append((ind)[:])\n",
    "    print('Appended indexes')\n",
    "#     for i,column in enumerate(process_sim.columns):\n",
    "#         print('Finding 20 most similar for column ',i)\n",
    "#         ind = list(process_sim.nlargest(20, column).index)\n",
    "#         f_list.append((ind)[:])\n",
    "#         process_sim = process_sim.drop(ind)\n",
    "    flat_list = [item for sublist in f_list for item in sublist]\n",
    "    print('Calculating neg_pos')\n",
    "    neg_pos = df1.loc[(df1.index.isin(flat_list))]\n",
    "    print('Calculated neg_pos')\n",
    "    return neg_pos\n",
    "# tran = np.transpose(similarity)\n",
    "# tran_df = pd.DataFrame(data = tran,index=df1[df1.fire==0].index,columns=df[df.fire==1].index)\n",
    "# tran_df.to_csv('/'+os.path.join(*filename.split('/')[:-1]+'/similarities.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year:2015\n",
      "Reading  0 /work2/pa21/sgirtsou/production/2015/05/may_2015_dummies.csv\n",
      "7  fires found\n",
      "Getting the dummies ......\n",
      "month_4was not in columns of/work2/pa21/sgirtsou/production/2015/05/may_2015_dummies.csv\n",
      "month_6was not in columns of/work2/pa21/sgirtsou/production/2015/05/may_2015_dummies.csv\n",
      "month_7was not in columns of/work2/pa21/sgirtsou/production/2015/05/may_2015_dummies.csv\n",
      "month_8was not in columns of/work2/pa21/sgirtsou/production/2015/05/may_2015_dummies.csv\n",
      "month_9was not in columns of/work2/pa21/sgirtsou/production/2015/05/may_2015_dummies.csv\n",
      "Starting normalization....\n",
      "max_temp\n",
      "min_temp\n",
      "mean_temp\n",
      "res_max\n",
      "dom_vel\n",
      "rain_7days\n",
      "dem\n",
      "slope\n",
      "curvature\n",
      "aspect\n",
      "ndvi_new\n",
      "evi\n",
      "lst_day\n",
      "lst_night\n",
      "max_dew_temp\n",
      "mean_dew_temp\n",
      "min_dew_temp\n",
      "Saving normalized dataset....\n",
      "0 / 1\n",
      "Finding similarity....\n",
      "Creating similarity dataframe\n",
      "similarity dataframe ok\n",
      "Found 140 largest similarities\n",
      "Appended indexes\n",
      "Calculating neg_pos\n",
      "Calculated neg_pos\n",
      "Reading  1 /work2/pa21/sgirtsou/production/2015/08/august_2015_dummies.csv\n",
      "144  fires found\n",
      "Getting the dummies ......\n",
      "month_4was not in columns of/work2/pa21/sgirtsou/production/2015/08/august_2015_dummies.csv\n",
      "month_5was not in columns of/work2/pa21/sgirtsou/production/2015/08/august_2015_dummies.csv\n",
      "month_6was not in columns of/work2/pa21/sgirtsou/production/2015/08/august_2015_dummies.csv\n",
      "month_7was not in columns of/work2/pa21/sgirtsou/production/2015/08/august_2015_dummies.csv\n",
      "month_9was not in columns of/work2/pa21/sgirtsou/production/2015/08/august_2015_dummies.csv\n",
      "Starting normalization....\n",
      "max_temp\n",
      "min_temp\n",
      "mean_temp\n",
      "res_max\n",
      "dom_vel\n",
      "rain_7days\n",
      "dem\n",
      "slope\n",
      "curvature\n",
      "aspect\n",
      "ndvi_new\n",
      "evi\n",
      "lst_day\n",
      "lst_night\n",
      "max_dew_temp\n",
      "mean_dew_temp\n",
      "min_dew_temp\n",
      "Saving normalized dataset....\n",
      "0 / 2\n",
      "Finding similarity....\n",
      "Creating similarity dataframe\n",
      "similarity dataframe ok\n",
      "Found 2000 largest similarities\n",
      "Appended indexes\n",
      "Calculating neg_pos\n",
      "Calculated neg_pos\n",
      "1 / 2\n",
      "Finding similarity....\n",
      "Creating similarity dataframe\n",
      "similarity dataframe ok\n",
      "Found 880 largest similarities\n",
      "Appended indexes\n",
      "Calculating neg_pos\n",
      "Calculated neg_pos\n",
      "Reading  2 /work2/pa21/sgirtsou/production/2015/09/september_2015_dummies.csv\n",
      "No Unnamed: 0.1.1 column\n",
      "206  fires found\n",
      "Getting the dummies ......\n",
      "month_4was not in columns of/work2/pa21/sgirtsou/production/2015/09/september_2015_dummies.csv\n",
      "month_5was not in columns of/work2/pa21/sgirtsou/production/2015/09/september_2015_dummies.csv\n",
      "month_6was not in columns of/work2/pa21/sgirtsou/production/2015/09/september_2015_dummies.csv\n",
      "month_7was not in columns of/work2/pa21/sgirtsou/production/2015/09/september_2015_dummies.csv\n",
      "month_8was not in columns of/work2/pa21/sgirtsou/production/2015/09/september_2015_dummies.csv\n",
      "Starting normalization....\n",
      "max_temp\n",
      "min_temp\n",
      "mean_temp\n",
      "res_max\n",
      "dom_vel\n",
      "rain_7days\n",
      "dem\n",
      "slope\n",
      "curvature\n",
      "aspect\n",
      "ndvi_new\n",
      "evi\n",
      "lst_day\n",
      "lst_night\n",
      "max_dew_temp\n",
      "mean_dew_temp\n",
      "min_dew_temp\n",
      "Saving normalized dataset....\n",
      "0 / 3\n",
      "Finding similarity....\n",
      "Creating similarity dataframe\n",
      "similarity dataframe ok\n",
      "Found 2000 largest similarities\n",
      "Appended indexes\n",
      "Calculating neg_pos\n",
      "Calculated neg_pos\n",
      "1 / 3\n",
      "Finding similarity....\n",
      "Creating similarity dataframe\n",
      "similarity dataframe ok\n",
      "Found 2000 largest similarities\n",
      "Appended indexes\n",
      "Calculating neg_pos\n",
      "Calculated neg_pos\n",
      "2 / 3\n",
      "Finding similarity....\n",
      "Creating similarity dataframe\n",
      "similarity dataframe ok\n",
      "Found 120 largest similarities\n",
      "Appended indexes\n",
      "Calculating neg_pos\n",
      "Calculated neg_pos\n",
      "Reading  3 /work2/pa21/sgirtsou/production/2015/04/april_2015_dummies.csv\n",
      "No Unnamed: 0.1.1 column\n",
      "0  fires found\n",
      "Reading  4 /work2/pa21/sgirtsou/production/2015/07/july_2015_dummies.csv\n",
      "No Unnamed: 0.1.1 column\n",
      "404  fires found\n",
      "Getting the dummies ......\n",
      "month_4was not in columns of/work2/pa21/sgirtsou/production/2015/07/july_2015_dummies.csv\n",
      "month_5was not in columns of/work2/pa21/sgirtsou/production/2015/07/july_2015_dummies.csv\n",
      "month_6was not in columns of/work2/pa21/sgirtsou/production/2015/07/july_2015_dummies.csv\n",
      "month_8was not in columns of/work2/pa21/sgirtsou/production/2015/07/july_2015_dummies.csv\n",
      "month_9was not in columns of/work2/pa21/sgirtsou/production/2015/07/july_2015_dummies.csv\n",
      "Starting normalization....\n",
      "max_temp\n",
      "min_temp\n",
      "mean_temp\n",
      "res_max\n",
      "dom_vel\n",
      "rain_7days\n",
      "dem\n",
      "slope\n",
      "curvature\n",
      "aspect\n",
      "ndvi_new\n",
      "evi\n",
      "lst_day\n",
      "lst_night\n",
      "max_dew_temp\n",
      "mean_dew_temp\n",
      "min_dew_temp\n",
      "Saving normalized dataset....\n",
      "0 / 5\n",
      "Finding similarity....\n",
      "Creating similarity dataframe\n",
      "similarity dataframe ok\n",
      "Found 2000 largest similarities\n",
      "Appended indexes\n",
      "Calculating neg_pos\n",
      "Calculated neg_pos\n",
      "1 / 5\n",
      "Finding similarity....\n",
      "Creating similarity dataframe\n",
      "similarity dataframe ok\n",
      "Found 2000 largest similarities\n",
      "Appended indexes\n",
      "Calculating neg_pos\n",
      "Calculated neg_pos\n",
      "2 / 5\n",
      "Finding similarity....\n",
      "Creating similarity dataframe\n",
      "similarity dataframe ok\n",
      "Found 2000 largest similarities\n",
      "Appended indexes\n",
      "Calculating neg_pos\n",
      "Calculated neg_pos\n",
      "3 / 5\n",
      "Finding similarity....\n",
      "Creating similarity dataframe\n",
      "similarity dataframe ok\n",
      "Found 2000 largest similarities\n",
      "Appended indexes\n",
      "Calculating neg_pos\n",
      "Calculated neg_pos\n",
      "4 / 5\n",
      "Finding similarity....\n",
      "Creating similarity dataframe\n",
      "similarity dataframe ok\n",
      "Found 80 largest similarities\n",
      "Appended indexes\n",
      "Calculating neg_pos\n",
      "Calculated neg_pos\n",
      "Reading  5 /work2/pa21/sgirtsou/production/2015/06/june_2015_dummies.csv\n",
      "No Unnamed: 0.1.1 column\n",
      "33  fires found\n",
      "Getting the dummies ......\n",
      "month_4was not in columns of/work2/pa21/sgirtsou/production/2015/06/june_2015_dummies.csv\n",
      "month_5was not in columns of/work2/pa21/sgirtsou/production/2015/06/june_2015_dummies.csv\n",
      "month_7was not in columns of/work2/pa21/sgirtsou/production/2015/06/june_2015_dummies.csv\n",
      "month_8was not in columns of/work2/pa21/sgirtsou/production/2015/06/june_2015_dummies.csv\n",
      "month_9was not in columns of/work2/pa21/sgirtsou/production/2015/06/june_2015_dummies.csv\n",
      "Starting normalization....\n",
      "max_temp\n",
      "min_temp\n",
      "mean_temp\n",
      "res_max\n",
      "dom_vel\n",
      "rain_7days\n",
      "dem\n",
      "slope\n",
      "curvature\n",
      "aspect\n",
      "ndvi_new\n",
      "evi\n",
      "lst_day\n",
      "lst_night\n",
      "max_dew_temp\n",
      "mean_dew_temp\n",
      "min_dew_temp\n",
      "Saving normalized dataset....\n",
      "0 / 1\n",
      "Finding similarity....\n",
      "Creating similarity dataframe\n",
      "similarity dataframe ok\n",
      "Found 660 largest similarities\n",
      "Appended indexes\n",
      "Calculating neg_pos\n",
      "Calculated neg_pos\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(dict.keys()):\n",
    "    dataset = DataFrame()\n",
    "    print('Year:'+key)\n",
    "    for i, filename in enumerate(dict[key]):\n",
    "        f_list=[]\n",
    "        print('Reading ',i, filename)\n",
    "        df = pd.read_csv(filename)\n",
    "        try:\n",
    "            df = df.drop(['Unnamed: 0'],axis=1)\n",
    "        except:\n",
    "            print('No Unnamed: 0.1.1 column')\n",
    "        df = df.fillna(0)\n",
    "        n_fires = df[df.fire==1]['id'].count()\n",
    "        print(n_fires,' fires found')\n",
    "        if n_fires == 0:\n",
    "            continue\n",
    "        else:\n",
    "            df['wkd'] = pd.to_datetime(df['firedate'],format='%Y%m%d').dt.dayofweek\n",
    "            df['month'] = pd.to_datetime(df['firedate'],format='%Y%m%d').dt.month\n",
    "            print('Getting the dummies ......')\n",
    "            df1 = df.fillna(0)\n",
    "            df1 = pd.get_dummies(df, columns=['wkd', 'month'])\n",
    "            del df\n",
    "            months = ['month_4','month_5','month_6','month_7','month_8','month_9']\n",
    "            for m in months:\n",
    "                if m not in df1.columns:\n",
    "                    print(m+ 'was not in columns of' + filename)\n",
    "                    df1[m] = 0   \n",
    "            print('Starting normalization....')\n",
    "            df1 = normalize_dataset(df1,d)\n",
    "            print('Saving normalized dataset....')\n",
    "            #df1.to_csv(filename[:-12]+'_norm.csv')\n",
    "            fires = df1[df1.fire == 1]\n",
    "            nofires = df1[df1.fire == 0]\n",
    "            for j in range(0,math.ceil((n_fires)/100)):\n",
    "                print(j,'/',math.ceil((n_fires)/100))\n",
    "                fires_=fires.iloc[j*100:(j+1)*100]\n",
    "                neg_pos = create_dataset(fires_,nofires,df1)\n",
    "                if i ==0:\n",
    "                    neg_pos.to_csv('/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/balanced_'+key+'_new.csv')\n",
    "                    i = 1\n",
    "                else:\n",
    "                    neg_pos.to_csv('/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/balanced_'+key+'_new.csv',mode='a', header=False)\n",
    "                #dataset = dataset.append(neg_pos,ignore_index=True)\n",
    "            df1[df1.fire == 1].to_csv('/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/balanced_'+key+'_new.csv',mode='a', header=False)\n",
    "            #dataset = dataset.append(df1[df1.fire == 1],ignore_index=True)\n",
    "            #print(dataset[dataset.fire==1]['id'].count(),'fires in dataset')\n",
    "    #dataset.to_csv('/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/balanced_'+key+'.csv',mode='a', header=False)\n",
    "#df1.to_csv(filename[:-12]+'_norm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Year:2015\n",
      "Reading  0 /work2/pa21/sgirtsou/production/2015/05/may_2015_dummies.csv\n",
      "77\n",
      "Reading  1 /work2/pa21/sgirtsou/production/2015/08/august_2015_dummies.csv\n",
      "77\n",
      "Reading  2 /work2/pa21/sgirtsou/production/2015/09/september_2015_dummies.csv\n",
      "76\n",
      "Reading  3 /work2/pa21/sgirtsou/production/2015/04/april_2015_dummies.csv\n",
      "76\n",
      "Reading  4 /work2/pa21/sgirtsou/production/2015/07/july_2015_dummies.csv\n",
      "76\n",
      "Reading  5 /work2/pa21/sgirtsou/production/2015/06/june_2015_dummies.csv\n",
      "76\n"
     ]
    }
   ],
   "source": [
    "for key in sorted(dict.keys()):\n",
    "    print('Year:'+key)\n",
    "    for i, filename in enumerate(dict[key]):\n",
    "        print('Reading ',i, filename)\n",
    "        df = pd.read_csv(filename,nrows=1)\n",
    "        #print(df.columns)\n",
    "        print(len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/work2/pa21/sgirtsou/production/2015/05/may_2015_dummies.csv',nrows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'firedate', 'max_temp', 'min_temp', 'mean_temp',\n",
       "       'res_max', 'dom_vel', 'rain_7days', 'dem', 'slope', 'curvature',\n",
       "       'aspect', 'ndvi_new', 'evi', 'lst_day', 'lst_night', 'max_dew_temp',\n",
       "       'mean_dew_temp', 'min_dew_temp', 'fire', 'dir_max_1', 'dir_max_2',\n",
       "       'dir_max_3', 'dir_max_4', 'dir_max_5', 'dir_max_6', 'dir_max_7',\n",
       "       'dir_max_8', 'dom_dir_1', 'dom_dir_2', 'dom_dir_3', 'dom_dir_4',\n",
       "       'dom_dir_5', 'dom_dir_6', 'dom_dir_7', 'dom_dir_8', 'corine_111',\n",
       "       'corine_112', 'corine_121', 'corine_122', 'corine_123', 'corine_124',\n",
       "       'corine_131', 'corine_132', 'corine_133', 'corine_141', 'corine_142',\n",
       "       'corine_211', 'corine_212', 'corine_213', 'corine_221', 'corine_222',\n",
       "       'corine_223', 'corine_231', 'corine_241', 'corine_242', 'corine_243',\n",
       "       'corine_244', 'corine_311', 'corine_312', 'corine_313', 'corine_321',\n",
       "       'corine_322', 'corine_323', 'corine_324', 'corine_331', 'corine_332',\n",
       "       'corine_333', 'corine_334', 'corine_411', 'corine_412', 'corine_421',\n",
       "       'corine_422', 'corine_511', 'corine_512', 'corine_521'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('/work2/pa21/sgirtsou/production/2013/06/june_2013_dummies.csv')/work2/pa21/sgirtsou/production/2013/06/june_2013_dummies.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
