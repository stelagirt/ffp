{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, KFold, cross_val_score, RandomizedSearchCV, GridSearchCV, GroupKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, confusion_matrix\n",
    "import time\n",
    "from sklearn.metrics import precision_score, make_scorer,recall_score,f1_score,roc_auc_score\n",
    "import warnings\n",
    "import csv\n",
    "from os import listdir, chdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hyperparameter_tune(base_model, parameters, kfold, X, y, groups):\n",
    "    start_time = time.time()\n",
    "    k = GroupKFold(n_splits=kfold)\n",
    "\n",
    "    prec_1 = make_scorer(precision_score, pos_label=1)\n",
    "    rec_1 = make_scorer(recall_score, pos_label=1)\n",
    "    f1_1 = make_scorer(f1_score, pos_label=1)\n",
    "    roc = make_scorer(roc_auc_score)\n",
    "    prec_0 = make_scorer(precision_score, pos_label=0)\n",
    "    rec_0 = make_scorer(recall_score, pos_label=0)\n",
    "    f1_0 = make_scorer(f1_score, pos_label=0)\n",
    "\n",
    "    scoring_st = {'prec_1': prec_1, 'rec_1': rec_1, 'f1_1': f1_1, 'roc': roc, 'prec_0': prec_0, 'rec_0': rec_0,\n",
    "              'f1_0': f1_0}\n",
    "\n",
    "    optimal_model = RandomizedSearchCV(base_model,\n",
    "                                      param_distributions=parameters,\n",
    "                                      n_iter=200,\n",
    "                                      cv=k,\n",
    "                                      scoring = scoring_st,\n",
    "                                      n_jobs=10,\n",
    "                                      refit='rec_1',\n",
    "                                      verbose=3,\n",
    "                                      return_train_score=True)\n",
    "                                      #random_state=SEED)\n",
    "\n",
    "    optimal_model.fit(X, y, groups)\n",
    "\n",
    "    stop_time = time.time()\n",
    "    #scores = cross_validate(optimal_model, X, y, cv=k, scoring= scoring_st, return_train_score=True, return_estimator=True)\n",
    "    print(\"Elapsed Time:\", time.strftime(\"%H:%M:%S\", time.gmtime(stop_time - start_time)))\n",
    "    print(\"====================\")\n",
    "    #print(\"Cross Val Mean: {:.3f}, Cross Val Stdev: {:.3f}\".format(scores.mean(), scores.std()))\n",
    "    print(\"Best Score: {:.3f}\".format(optimal_model.best_score_))\n",
    "    print(\"Best Parameters: {}\".format(optimal_model.best_params_))\n",
    "    return optimal_model.best_params_, optimal_model.best_score_, optimal_model.cv_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mypath = '/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "allfiles = [f for f in listdir(mypath) if f.endswith('norm.csv') and f[0].isdigit()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016_norm.csv',\n",
       " '2015_norm.csv',\n",
       " '2014_norm.csv',\n",
       " '2017_norm.csv',\n",
       " '2012_norm.csv',\n",
       " '2011_norm.csv',\n",
       " '2010_norm.csv',\n",
       " '2018_norm.csv',\n",
       " '2013_norm.csv']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chdir(mypath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "li = []\n",
    "for filename in allfiles:\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    li.append(df)\n",
    "    \n",
    "frame = pd.concat(li, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame.firedate = pd.to_datetime(frame.firedate).dt.strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_part = frame[['id', 'firedate', 'max_temp', 'min_temp', 'mean_temp','res_max', 'dom_vel', 'rain_7days', 'dem', 'slope', 'curvature',\n",
    "       'aspect', 'ndvi_new', 'evi', 'lst_day', 'lst_night', 'max_dew_temp','mean_dew_temp', 'min_dew_temp', 'fire', 'dir_max_1', 'dir_max_2',\n",
    "       'dir_max_3', 'dir_max_4', 'dir_max_5', 'dir_max_6', 'dir_max_7','dir_max_8', 'dom_dir_1', 'dom_dir_2', 'dom_dir_3', 'dom_dir_4',\n",
    "       'dom_dir_5', 'dom_dir_6', 'dom_dir_7', 'dom_dir_8', 'corine_111','corine_112', 'corine_121', 'corine_122', 'corine_123', 'corine_124',\n",
    "       'corine_131', 'corine_132', 'corine_133', 'corine_141', 'corine_142','corine_211', 'corine_212', 'corine_213', 'corine_221', 'corine_222',\n",
    "       'corine_223', 'corine_231', 'corine_241', 'corine_242', 'corine_243','corine_244', 'corine_311', 'corine_312', 'corine_313', 'corine_321',\n",
    "       'corine_322', 'corine_323', 'corine_324', 'corine_331', 'corine_332','corine_333', 'corine_334', 'corine_411', 'corine_412', 'corine_421',\n",
    "       'corine_422', 'corine_511', 'corine_512', 'corine_521', 'wkd_0','wkd_1', 'wkd_2', 'wkd_3', 'wkd_4', 'wkd_5', 'wkd_6', 'month_5',\n",
    "       'month_6', 'month_7', 'month_8', 'month_9', 'month_4', 'frequency','f81', 'x', 'y']].copy()\n",
    "\n",
    "X_unnorm, y_int = df_part[['max_temp', 'min_temp', 'mean_temp','res_max', 'dom_vel', 'rain_7days', 'dem', 'slope', 'curvature',\n",
    "       'aspect', 'ndvi_new', 'evi', 'lst_day', 'lst_night', 'max_dew_temp','mean_dew_temp', 'min_dew_temp', 'dir_max_1', 'dir_max_2',\n",
    "       'dir_max_3', 'dir_max_4', 'dir_max_5', 'dir_max_6', 'dir_max_7','dir_max_8', 'dom_dir_1', 'dom_dir_2', 'dom_dir_3', 'dom_dir_4',\n",
    "       'dom_dir_5', 'dom_dir_6', 'dom_dir_7', 'dom_dir_8', 'corine_111','corine_112', 'corine_121', 'corine_122', 'corine_123', 'corine_124',\n",
    "       'corine_131', 'corine_132', 'corine_133', 'corine_141', 'corine_142','corine_211', 'corine_212', 'corine_213', 'corine_221', 'corine_222',\n",
    "       'corine_223', 'corine_231', 'corine_241', 'corine_242', 'corine_243','corine_244', 'corine_311', 'corine_312', 'corine_313', 'corine_321',\n",
    "       'corine_322', 'corine_323', 'corine_324', 'corine_331', 'corine_332','corine_333', 'corine_334', 'corine_411', 'corine_412', 'corine_421',\n",
    "       'corine_422', 'corine_511', 'corine_512', 'corine_521', 'wkd_0','wkd_1', 'wkd_2', 'wkd_3', 'wkd_4', 'wkd_5', 'wkd_6', 'month_5',\n",
    "       'month_6', 'month_7', 'month_8', 'month_9', 'month_4', 'frequency','f81', 'x', 'y']], df_part['fire']\n",
    "\n",
    "groups = frame['firedate']\n",
    "\n",
    "#X = normalize_dataset(X_unnorm, 'std')\n",
    "y = y_int\n",
    "\n",
    "X_ = X_unnorm.values\n",
    "#X_ = X.values\n",
    "y_ = y.values\n",
    "groupskfold = groups.values\n",
    "\n",
    "rf = RandomForestClassifier(n_jobs=-1)\n",
    "depth = [10, 20, 100, 200, 400,500, 700, 1000, 1200,2000, None]\n",
    "n_estimators = [50, 100, 120, 150,170,200, 250, 350, 500, 750, 1000,1400, 1500]\n",
    "min_samples_split = [2, 10, 50, 70,100,120,150,180, 200, 250,400,600,1000, 1300, 2000]\n",
    "min_samples_leaf = [1, 10,30,40,50,100,120,150] #with numbers\n",
    "max_features = list(range(1,X_.shape[1]))\n",
    "bootstrap = [True, False]\n",
    "criterion = [\"gini\", \"entropy\"]\n",
    "class_weights = [{0:1,1:9},{0:1,1:300},{0:1,1:400},{0:1,1:500},{0:1,1:1000}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "cv =  10\n",
      "Fitting 10 folds for each of 200 candidates, totalling 2000 fits\n",
      "[CV] n_estimators=200, min_samples_split=180, min_samples_leaf=50, max_features=25, max_depth=100, criterion=gini, class_weight={0: 1, 1: 9}, bootstrap=False \n",
      "[CV] n_estimators=200, min_samples_split=180, min_samples_leaf=50, max_features=25, max_depth=100, criterion=gini, class_weight={0: 1, 1: 9}, bootstrap=False \n",
      "[CV] n_estimators=200, min_samples_split=180, min_samples_leaf=50, max_features=25, max_depth=100, criterion=gini, class_weight={0: 1, 1: 9}, bootstrap=False \n",
      "[CV] n_estimators=200, min_samples_split=180, min_samples_leaf=50, max_features=25, max_depth=100, criterion=gini, class_weight={0: 1, 1: 9}, bootstrap=False \n",
      "[CV] n_estimators=200, min_samples_split=180, min_samples_leaf=50, max_features=25, max_depth=100, criterion=gini, class_weight={0: 1, 1: 9}, bootstrap=False \n",
      "[CV] n_estimators=200, min_samples_split=180, min_samples_leaf=50, max_features=25, max_depth=100, criterion=gini, class_weight={0: 1, 1: 9}, bootstrap=False \n",
      "[CV] n_estimators=200, min_samples_split=180, min_samples_leaf=50, max_features=25, max_depth=100, criterion=gini, class_weight={0: 1, 1: 9}, bootstrap=False \n",
      "[CV] n_estimators=200, min_samples_split=180, min_samples_leaf=50, max_features=25, max_depth=100, criterion=gini, class_weight={0: 1, 1: 9}, bootstrap=False \n",
      "[CV] n_estimators=200, min_samples_split=180, min_samples_leaf=50, max_features=25, max_depth=100, criterion=gini, class_weight={0: 1, 1: 9}, bootstrap=False \n",
      "[CV] n_estimators=150, min_samples_split=150, min_samples_leaf=10, max_features=65, max_depth=100, criterion=gini, class_weight={0: 1, 1: 500}, bootstrap=True \n",
      "[CV] n_estimators=150, min_samples_split=150, min_samples_leaf=10, max_features=65, max_depth=100, criterion=gini, class_weight={0: 1, 1: 500}, bootstrap=True \n",
      "[CV] n_estimators=150, min_samples_split=150, min_samples_leaf=10, max_features=65, max_depth=100, criterion=gini, class_weight={0: 1, 1: 500}, bootstrap=True \n",
      "[CV] n_estimators=150, min_samples_split=150, min_samples_leaf=10, max_features=65, max_depth=100, criterion=gini, class_weight={0: 1, 1: 500}, bootstrap=True \n",
      "[CV] n_estimators=150, min_samples_split=150, min_samples_leaf=10, max_features=65, max_depth=100, criterion=gini, class_weight={0: 1, 1: 500}, bootstrap=True \n",
      "[CV] n_estimators=150, min_samples_split=150, min_samples_leaf=10, max_features=65, max_depth=100, criterion=gini, class_weight={0: 1, 1: 500}, bootstrap=True \n",
      "[CV] n_estimators=150, min_samples_split=150, min_samples_leaf=10, max_features=65, max_depth=100, criterion=gini, class_weight={0: 1, 1: 500}, bootstrap=True \n",
      "[CV]  n_estimators=200, min_samples_split=180, min_samples_leaf=50, max_features=25, max_depth=100, criterion=gini, class_weight={0: 1, 1: 9}, bootstrap=False, prec_1=0.6352257487706751, rec_1=0.9786501377410468, f1_1=0.7703984819734345, roc=0.719840260416098, prec_0=0.9574759945130316, rec_0=0.4610303830911493, f1_0=0.6223807400802497, total=   4.3s\n",
      "[CV] n_estimators=750, min_samples_split=120, min_samples_leaf=10, max_features=59, max_depth=20, criterion=entropy, class_weight={0: 1, 1: 9}, bootstrap=True \n"
     ]
    },
    {
     "ename": "JoblibRuntimeError",
     "evalue": "JoblibRuntimeError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7fb2d2d6a1e0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/apps/applications/python/anaconda3/5.0.1/lib/py...ges/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/apps/applic.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7fb2d2d6a1e0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/apps/applications/python/anaconda3/5.0.1/lib/py...ges/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/apps/applic.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2021, 4, 12, 12, 38, 21, 586289, tzinfo=tzutc()), 'msg_id': '6A4618395CCC4412971D89EF39B98536', 'msg_type': 'execute_request', 'session': 'C5862273C9D244BCA09FCFD7ABDD778B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '6A4618395CCC4412971D89EF39B98536', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'C5862273C9D244BCA09FCFD7ABDD778B']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2021, 4, 12, 12, 38, 21, 586289, tzinfo=tzutc()), 'msg_id': '6A4618395CCC4412971D89EF39B98536', 'msg_type': 'execute_request', 'session': 'C5862273C9D244BCA09FCFD7ABDD778B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '6A4618395CCC4412971D89EF39B98536', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'C5862273C9D244BCA09FCFD7ABDD778B'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2021, 4, 12, 12, 38, 21, 586289, tzinfo=tzutc()), 'msg_id': '6A4618395CCC4412971D89EF39B98536', 'msg_type': 'execute_request', 'session': 'C5862273C9D244BCA09FCFD7ABDD778B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '6A4618395CCC4412971D89EF39B98536', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')',), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')',)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')', store_history=True, silent=False, shell_futures=True)\n   2693                 self.displayhook.exec_result = result\n   2694 \n   2695                 # Execute the user code\n   2696                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2697                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2698                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2699                 \n   2700                 self.last_execution_succeeded = not has_raised\n   2701 \n   2702                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.For object>], cell_name='<ipython-input-10-ab11823add8f>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7fb282a16320, executi..._before_exec=None error_in_exec=None result=None>)\n   2797 \n   2798         try:\n   2799             for i, node in enumerate(to_run_exec):\n   2800                 mod = ast.Module([node])\n   2801                 code = compiler(mod, cell_name, \"exec\")\n-> 2802                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7fb282a18300, file \"<ipython-input-10-ab11823add8f>\", line 25>\n        result = <ExecutionResult object at 7fb282a16320, executi..._before_exec=None error_in_exec=None result=None>\n   2803                     return True\n   2804 \n   2805             for i, node in enumerate(to_run_interactive):\n   2806                 mod = ast.Interactive([node])\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7fb282a18300, file \"<ipython-input-10-ab11823add8f>\", line 25>, result=<ExecutionResult object at 7fb282a16320, executi..._before_exec=None error_in_exec=None result=None>)\n   2857         outflag = True  # happens in more places, so it's easier as default\n   2858         try:\n   2859             try:\n   2860                 self.hooks.pre_run_code_hook()\n   2861                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2862                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7fb282a18300, file \"<ipython-input-10-ab11823add8f>\", line 25>\n        self.user_global_ns = {'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'GroupKFold': <class 'sklearn.model_selection._split.GroupKFold'>, 'In': ['', 'import numpy as np\\nimport pandas as pd\\nfrom skle...t listdir, chdir\\nfrom os.path import isfile, join', 'def hyperparameter_tune(base_model, parameters, ...timal_model.best_score_, optimal_model.cv_results', \"mypath = '/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity'\", \"allfiles = [f for f in listdir(mypath) if f.endswith('norm.csv') and f[0].isdigit()]\", 'allfiles', 'chdir(mypath)', 'li = []\\nfor filename in allfiles:\\n    df = pd.re...\\nframe = pd.concat(li, axis=0, ignore_index=True)', \"frame.firedate = pd.to_datetime(frame.firedate).dt.strftime('%Y%m%d')\", \"df_part = frame[['id', 'firedate', 'max_temp', '...{0:1,1:300},{0:1,1:400},{0:1,1:500},{0:1,1:1000}]\", 'lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')'], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {5: ['2016_norm.csv', '2015_norm.csv', '2014_norm.csv', '2017_norm.csv', '2012_norm.csv', '2011_norm.csv', '2010_norm.csv', '2018_norm.csv', '2013_norm.csv']}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'StandardScaler': <class 'sklearn.preprocessing.data.StandardScaler'>, 'StratifiedKFold': <class 'sklearn.model_selection._split.StratifiedKFold'>, 'X_': array([[ 0.652     ,  0.797     ,  0.728     , ....  0.09141531,\n         0.74536127,  0.90534205]]), ...}\n        self.user_ns = {'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'GroupKFold': <class 'sklearn.model_selection._split.GroupKFold'>, 'In': ['', 'import numpy as np\\nimport pandas as pd\\nfrom skle...t listdir, chdir\\nfrom os.path import isfile, join', 'def hyperparameter_tune(base_model, parameters, ...timal_model.best_score_, optimal_model.cv_results', \"mypath = '/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity'\", \"allfiles = [f for f in listdir(mypath) if f.endswith('norm.csv') and f[0].isdigit()]\", 'allfiles', 'chdir(mypath)', 'li = []\\nfor filename in allfiles:\\n    df = pd.re...\\nframe = pd.concat(li, axis=0, ignore_index=True)', \"frame.firedate = pd.to_datetime(frame.firedate).dt.strftime('%Y%m%d')\", \"df_part = frame[['id', 'firedate', 'max_temp', '...{0:1,1:300},{0:1,1:400},{0:1,1:500},{0:1,1:1000}]\", 'lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')'], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {5: ['2016_norm.csv', '2015_norm.csv', '2014_norm.csv', '2017_norm.csv', '2012_norm.csv', '2011_norm.csv', '2010_norm.csv', '2018_norm.csv', '2013_norm.csv']}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'StandardScaler': <class 'sklearn.preprocessing.data.StandardScaler'>, 'StratifiedKFold': <class 'sklearn.model_selection._split.StratifiedKFold'>, 'X_': array([[ 0.652     ,  0.797     ,  0.728     , ....  0.09141531,\n         0.74536127,  0.90534205]]), ...}\n   2863             finally:\n   2864                 # Reset our crash handler in place\n   2865                 sys.excepthook = old_excepthook\n   2866         except SystemExit as e:\n\n...........................................................................\n/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/<ipython-input-10-ab11823add8f> in <module>()\n     23 results = pd.DataFrame(columns=columns_sel)\n     24 \n     25 for i in folds:\n     26     print(\"\\ncv = \", i)\n     27     start = time.time()\n---> 28     best_params, best_score, full_scores = hyperparameter_tune(rf, lots_of_parameters, i, X_, y_, groupskfold)\n     29 \n     30     df_results = pd.DataFrame.from_dict(full_scores)\n     31     df_results['folds'] = int(i)\n     32     #df_results.to_csv('/home/sgirtsou/Documents/GridSearchCV/RF/RFcv_25kbalanced_noshufflestrictcriterion.csv')\n\n...........................................................................\n/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/<ipython-input-2-fa568b5055a5> in hyperparameter_tune(base_model=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), parameters={'bootstrap': [True, False], 'class_weight': [{0: 1, 1: 9}, {0: 1, 1: 300}, {0: 1, 1: 400}, {0: 1, 1: 500}, {0: 1, 1: 1000}], 'criterion': ['gini', 'entropy'], 'max_depth': [10, 20, 100, 200, 400, 500, 700, 1000, 1200, 2000, None], 'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...], 'min_samples_leaf': [1, 10, 30, 40, 50, 100, 120, 150], 'min_samples_split': [2, 10, 50, 70, 100, 120, 150, 180, 200, 250, 400, 600, 1000, 1300, 2000], 'n_estimators': [50, 100, 120, 150, 170, 200, 250, 350, 500, 750, 1000, 1400, 1500]}, kfold=10, X=array([[ 0.652     ,  0.797     ,  0.728     , ....  0.09141531,\n         0.74536127,  0.90534205]]), y=array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), groups=array(['20160822', '20160822', '20160721', ..., ...8', '20130805',\n       '20130727'], dtype=object))\n     22                                       refit='rec_1',\n     23                                       verbose=3,\n     24                                       return_train_score=True)\n     25                                       #random_state=SEED)\n     26 \n---> 27     optimal_model.fit(X, y, groups)\n     28 \n     29     stop_time = time.time()\n     30     #scores = cross_validate(optimal_model, X, y, cv=k, scoring= scoring_st, return_train_score=True, return_estimator=True)\n     31     print(\"Elapsed Time:\", time.strftime(\"%H:%M:%S\", time.gmtime(stop_time - start_time)))\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/model_selection/_search.py in fit(self=RandomizedSearchCV(cv=GroupKFold(n_splits=10), e...rer(f1_score, pos_label=0)},\n          verbose=3), X=array([[ 0.652     ,  0.797     ,  0.728     , ....  0.09141531,\n         0.74536127,  0.90534205]]), y=array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), groups=array(['20160822', '20160822', '20160721', ..., ...8', '20130805',\n       '20130727'], dtype=object), **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method _BaseKFold.split of GroupKFold(n_splits=10)>\n        X = array([[ 0.652     ,  0.797     ,  0.728     , ....  0.09141531,\n         0.74536127,  0.90534205]])\n        y = array([ 1.,  1.,  1., ...,  0.,  0.,  0.])\n        groups = array(['20160822', '20160822', '20160721', ..., ...8', '20130805',\n       '20130727'], dtype=object)\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=10), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=10)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nRuntimeError                                       Mon Apr 12 15:38:22 2021\nPID: 2258Python 3.6.3: /apps/applications/python/anaconda3/5.0.1/bin/python\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]]), array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), {'f1_0': make_scorer(f1_score, pos_label=0), 'f1_1': make_scorer(f1_score, pos_label=1), 'prec_0': make_scorer(precision_score, pos_label=0), 'prec_1': make_scorer(precision_score, pos_label=1), 'rec_0': make_scorer(recall_score, pos_label=0), 'rec_1': make_scorer(recall_score, pos_label=1), 'roc': make_scorer(roc_auc_score)}, array([    0,     1,     2, ..., 29654, 29655, 29656]), array([   13,    14,    15, ..., 29594, 29604, 29608]), 3, {'bootstrap': False, 'class_weight': {0: 1, 1: 9}, 'criterion': 'gini', 'max_depth': 100, 'max_features': 25, 'min_samples_leaf': 50, 'min_samples_split': 180, 'n_estimators': 200}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]]), array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), {'f1_0': make_scorer(f1_score, pos_label=0), 'f1_1': make_scorer(f1_score, pos_label=1), 'prec_0': make_scorer(precision_score, pos_label=0), 'prec_1': make_scorer(precision_score, pos_label=1), 'rec_0': make_scorer(recall_score, pos_label=0), 'rec_1': make_scorer(recall_score, pos_label=1), 'roc': make_scorer(roc_auc_score)}, array([    0,     1,     2, ..., 29654, 29655, 29656]), array([   13,    14,    15, ..., 29594, 29604, 29608]), 3, {'bootstrap': False, 'class_weight': {0: 1, 1: 9}, 'criterion': 'gini', 'max_depth': 100, 'max_features': 25, 'min_samples_leaf': 50, 'min_samples_split': 180, 'n_estimators': 200})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), X=memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]]), y=array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), scorer={'f1_0': make_scorer(f1_score, pos_label=0), 'f1_1': make_scorer(f1_score, pos_label=1), 'prec_0': make_scorer(precision_score, pos_label=0), 'prec_1': make_scorer(precision_score, pos_label=1), 'rec_0': make_scorer(recall_score, pos_label=0), 'rec_1': make_scorer(recall_score, pos_label=1), 'roc': make_scorer(roc_auc_score)}, train=array([    0,     1,     2, ..., 29654, 29655, 29656]), test=array([   13,    14,    15, ..., 29594, 29604, 29608]), verbose=3, parameters={'bootstrap': False, 'class_weight': {0: 1, 1: 9}, 'criterion': 'gini', 'max_depth': 100, 'max_features': 25, 'min_samples_leaf': 50, 'min_samples_split': 180, 'n_estimators': 200}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas... random_state=None, verbose=0, warm_start=False)>\n        X_train = memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]])\n        y_train = array([ 1.,  1.,  1., ...,  0.,  0.,  0.])\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/ensemble/forest.py in fit(self=RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), X=array([[ 0.65200001,  0.79699999,  0.72799999, ....        0.74536127,  0.90534204]], dtype=float32), y=array([[ 1.],\n       [ 1.],\n       [ 1.],\n       ..., \n       [ 0.],\n       [ 0.],\n       [ 0.]]), sample_weight=array([ 9.,  9.,  9., ...,  1.,  1.,  1.]))\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 199\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    744             raise ValueError('This Parallel instance is already running')\n    745         # A flag used to abort the dispatching of jobs in case an\n    746         # exception is found\n    747         self._aborting = False\n    748         if not self._managed_backend:\n--> 749             n_jobs = self._initialize_backend()\n        n_jobs = undefined\n        self._initialize_backend = <bound method Parallel._initialize_backend of Parallel(n_jobs=-1)>\n    750         else:\n    751             n_jobs = self._effective_n_jobs()\n    752 \n    753         iterator = iter(iterable)\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _initialize_backend(self=Parallel(n_jobs=-1))\n    542 \n    543     def _initialize_backend(self):\n    544         \"\"\"Build a process or thread pool and return the number of workers\"\"\"\n    545         try:\n    546             n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n--> 547                                              **self._backend_args)\n        self._backend_args = {'context': <multiprocessing.context.ForkContext object>, 'max_nbytes': 1048576, 'mmap_mode': 'r', 'temp_folder': None, 'verbose': 0}\n    548             if self.timeout is not None and not self._backend.supports_timeout:\n    549                 warnings.warn(\n    550                     'The backend class {!r} does not support timeout. '\n    551                     \"You have set 'timeout={}' in Parallel but \"\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in configure(self=<sklearn.externals.joblib._parallel_backends.ThreadingBackend object>, n_jobs=32, parallel=Parallel(n_jobs=-1), **backend_args={'context': <multiprocessing.context.ForkContext object>, 'max_nbytes': 1048576, 'mmap_mode': 'r', 'temp_folder': None, 'verbose': 0})\n    245         n_jobs = self.effective_n_jobs(n_jobs)\n    246         if n_jobs == 1:\n    247             # Avoid unnecessary overhead and use sequential backend instead.\n    248             raise FallbackToBackend(SequentialBackend())\n    249         self.parallel = parallel\n--> 250         self._pool = ThreadPool(n_jobs)\n        self._pool = undefined\n        n_jobs = 32\n    251         return n_jobs\n    252 \n    253 \n    254 class MultiprocessingBackend(PoolManagerMixin, AutoBatchingMixin,\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py in __init__(self=<multiprocessing.pool.ThreadPool object>, processes=32, initializer=None, initargs=())\n    784     def Process(*args, **kwds):\n    785         from .dummy import Process\n    786         return Process(*args, **kwds)\n    787 \n    788     def __init__(self, processes=None, initializer=None, initargs=()):\n--> 789         Pool.__init__(self, processes, initializer, initargs)\n        self = <multiprocessing.pool.ThreadPool object>\n        processes = 32\n        initializer = None\n        initargs = ()\n    790 \n    791     def _setup_queues(self):\n    792         self._inqueue = queue.Queue()\n    793         self._outqueue = queue.Queue()\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py in __init__(self=<multiprocessing.pool.ThreadPool object>, processes=32, initializer=None, initargs=(), maxtasksperchild=None, context=None)\n    169         if initializer is not None and not callable(initializer):\n    170             raise TypeError('initializer must be a callable')\n    171 \n    172         self._processes = processes\n    173         self._pool = []\n--> 174         self._repopulate_pool()\n        self._repopulate_pool = <bound method Pool._repopulate_pool of <multiprocessing.pool.ThreadPool object>>\n    175 \n    176         self._worker_handler = threading.Thread(\n    177             target=Pool._handle_workers,\n    178             args=(self, )\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py in _repopulate_pool(self=<multiprocessing.pool.ThreadPool object>)\n    234                                    self._wrap_exception)\n    235                             )\n    236             self._pool.append(w)\n    237             w.name = w.name.replace('Process', 'PoolWorker')\n    238             w.daemon = True\n--> 239             w.start()\n        w.start = <bound method DummyProcess.start of <DummyProcess(Thread-16, initial daemon)>>\n    240             util.debug('added worker')\n    241 \n    242     def _maintain_pool(self):\n    243         \"\"\"Clean up any exited workers and start replacements for them.\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/dummy/__init__.py in start(self=<DummyProcess(Thread-16, initial daemon)>)\n     43     def start(self):\n     44         assert self._parent is current_process()\n     45         self._start_called = True\n     46         if hasattr(self._parent, '_children'):\n     47             self._parent._children[self] = None\n---> 48         threading.Thread.start(self)\n        self = <DummyProcess(Thread-16, initial daemon)>\n     49 \n     50     @property\n     51     def exitcode(self):\n     52         if self._start_called and not self.is_alive():\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/threading.py in start(self=<DummyProcess(Thread-16, initial daemon)>)\n    841         if self._started.is_set():\n    842             raise RuntimeError(\"threads can only be started once\")\n    843         with _active_limbo_lock:\n    844             _limbo[self] = self\n    845         try:\n--> 846             _start_new_thread(self._bootstrap, ())\n        self._bootstrap = <bound method Thread._bootstrap of <DummyProcess(Thread-16, initial daemon)>>\n    847         except Exception:\n    848             with _active_limbo_lock:\n    849                 del _limbo[self]\n    850             raise\n\nRuntimeError: can't start new thread\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\", line 458, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/ensemble/forest.py\", line 328, in fit\n    for i, t in enumerate(trees))\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 749, in __call__\n    n_jobs = self._initialize_backend()\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\", line 547, in _initialize_backend\n    **self._backend_args)\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 250, in configure\n    self._pool = ThreadPool(n_jobs)\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py\", line 789, in __init__\n    Pool.__init__(self, processes, initializer, initargs)\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py\", line 174, in __init__\n    self._repopulate_pool()\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py\", line 239, in _repopulate_pool\n    w.start()\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/dummy/__init__.py\", line 48, in start\n    threading.Thread.start(self)\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/threading.py\", line 846, in start\n    _start_new_thread(self._bootstrap, ())\nRuntimeError: can't start new thread\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nRuntimeError                                       Mon Apr 12 15:38:22 2021\nPID: 2258Python 3.6.3: /apps/applications/python/anaconda3/5.0.1/bin/python\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]]), array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), {'f1_0': make_scorer(f1_score, pos_label=0), 'f1_1': make_scorer(f1_score, pos_label=1), 'prec_0': make_scorer(precision_score, pos_label=0), 'prec_1': make_scorer(precision_score, pos_label=1), 'rec_0': make_scorer(recall_score, pos_label=0), 'rec_1': make_scorer(recall_score, pos_label=1), 'roc': make_scorer(roc_auc_score)}, array([    0,     1,     2, ..., 29654, 29655, 29656]), array([   13,    14,    15, ..., 29594, 29604, 29608]), 3, {'bootstrap': False, 'class_weight': {0: 1, 1: 9}, 'criterion': 'gini', 'max_depth': 100, 'max_features': 25, 'min_samples_leaf': 50, 'min_samples_split': 180, 'n_estimators': 200}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]]), array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), {'f1_0': make_scorer(f1_score, pos_label=0), 'f1_1': make_scorer(f1_score, pos_label=1), 'prec_0': make_scorer(precision_score, pos_label=0), 'prec_1': make_scorer(precision_score, pos_label=1), 'rec_0': make_scorer(recall_score, pos_label=0), 'rec_1': make_scorer(recall_score, pos_label=1), 'roc': make_scorer(roc_auc_score)}, array([    0,     1,     2, ..., 29654, 29655, 29656]), array([   13,    14,    15, ..., 29594, 29604, 29608]), 3, {'bootstrap': False, 'class_weight': {0: 1, 1: 9}, 'criterion': 'gini', 'max_depth': 100, 'max_features': 25, 'min_samples_leaf': 50, 'min_samples_split': 180, 'n_estimators': 200})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), X=memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]]), y=array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), scorer={'f1_0': make_scorer(f1_score, pos_label=0), 'f1_1': make_scorer(f1_score, pos_label=1), 'prec_0': make_scorer(precision_score, pos_label=0), 'prec_1': make_scorer(precision_score, pos_label=1), 'rec_0': make_scorer(recall_score, pos_label=0), 'rec_1': make_scorer(recall_score, pos_label=1), 'roc': make_scorer(roc_auc_score)}, train=array([    0,     1,     2, ..., 29654, 29655, 29656]), test=array([   13,    14,    15, ..., 29594, 29604, 29608]), verbose=3, parameters={'bootstrap': False, 'class_weight': {0: 1, 1: 9}, 'criterion': 'gini', 'max_depth': 100, 'max_features': 25, 'min_samples_leaf': 50, 'min_samples_split': 180, 'n_estimators': 200}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas... random_state=None, verbose=0, warm_start=False)>\n        X_train = memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]])\n        y_train = array([ 1.,  1.,  1., ...,  0.,  0.,  0.])\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/ensemble/forest.py in fit(self=RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), X=array([[ 0.65200001,  0.79699999,  0.72799999, ....        0.74536127,  0.90534204]], dtype=float32), y=array([[ 1.],\n       [ 1.],\n       [ 1.],\n       ..., \n       [ 0.],\n       [ 0.],\n       [ 0.]]), sample_weight=array([ 9.,  9.,  9., ...,  1.,  1.,  1.]))\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 199\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    744             raise ValueError('This Parallel instance is already running')\n    745         # A flag used to abort the dispatching of jobs in case an\n    746         # exception is found\n    747         self._aborting = False\n    748         if not self._managed_backend:\n--> 749             n_jobs = self._initialize_backend()\n        n_jobs = undefined\n        self._initialize_backend = <bound method Parallel._initialize_backend of Parallel(n_jobs=-1)>\n    750         else:\n    751             n_jobs = self._effective_n_jobs()\n    752 \n    753         iterator = iter(iterable)\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _initialize_backend(self=Parallel(n_jobs=-1))\n    542 \n    543     def _initialize_backend(self):\n    544         \"\"\"Build a process or thread pool and return the number of workers\"\"\"\n    545         try:\n    546             n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n--> 547                                              **self._backend_args)\n        self._backend_args = {'context': <multiprocessing.context.ForkContext object>, 'max_nbytes': 1048576, 'mmap_mode': 'r', 'temp_folder': None, 'verbose': 0}\n    548             if self.timeout is not None and not self._backend.supports_timeout:\n    549                 warnings.warn(\n    550                     'The backend class {!r} does not support timeout. '\n    551                     \"You have set 'timeout={}' in Parallel but \"\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in configure(self=<sklearn.externals.joblib._parallel_backends.ThreadingBackend object>, n_jobs=32, parallel=Parallel(n_jobs=-1), **backend_args={'context': <multiprocessing.context.ForkContext object>, 'max_nbytes': 1048576, 'mmap_mode': 'r', 'temp_folder': None, 'verbose': 0})\n    245         n_jobs = self.effective_n_jobs(n_jobs)\n    246         if n_jobs == 1:\n    247             # Avoid unnecessary overhead and use sequential backend instead.\n    248             raise FallbackToBackend(SequentialBackend())\n    249         self.parallel = parallel\n--> 250         self._pool = ThreadPool(n_jobs)\n        self._pool = undefined\n        n_jobs = 32\n    251         return n_jobs\n    252 \n    253 \n    254 class MultiprocessingBackend(PoolManagerMixin, AutoBatchingMixin,\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py in __init__(self=<multiprocessing.pool.ThreadPool object>, processes=32, initializer=None, initargs=())\n    784     def Process(*args, **kwds):\n    785         from .dummy import Process\n    786         return Process(*args, **kwds)\n    787 \n    788     def __init__(self, processes=None, initializer=None, initargs=()):\n--> 789         Pool.__init__(self, processes, initializer, initargs)\n        self = <multiprocessing.pool.ThreadPool object>\n        processes = 32\n        initializer = None\n        initargs = ()\n    790 \n    791     def _setup_queues(self):\n    792         self._inqueue = queue.Queue()\n    793         self._outqueue = queue.Queue()\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py in __init__(self=<multiprocessing.pool.ThreadPool object>, processes=32, initializer=None, initargs=(), maxtasksperchild=None, context=None)\n    169         if initializer is not None and not callable(initializer):\n    170             raise TypeError('initializer must be a callable')\n    171 \n    172         self._processes = processes\n    173         self._pool = []\n--> 174         self._repopulate_pool()\n        self._repopulate_pool = <bound method Pool._repopulate_pool of <multiprocessing.pool.ThreadPool object>>\n    175 \n    176         self._worker_handler = threading.Thread(\n    177             target=Pool._handle_workers,\n    178             args=(self, )\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py in _repopulate_pool(self=<multiprocessing.pool.ThreadPool object>)\n    234                                    self._wrap_exception)\n    235                             )\n    236             self._pool.append(w)\n    237             w.name = w.name.replace('Process', 'PoolWorker')\n    238             w.daemon = True\n--> 239             w.start()\n        w.start = <bound method DummyProcess.start of <DummyProcess(Thread-16, initial daemon)>>\n    240             util.debug('added worker')\n    241 \n    242     def _maintain_pool(self):\n    243         \"\"\"Clean up any exited workers and start replacements for them.\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/dummy/__init__.py in start(self=<DummyProcess(Thread-16, initial daemon)>)\n     43     def start(self):\n     44         assert self._parent is current_process()\n     45         self._start_called = True\n     46         if hasattr(self._parent, '_children'):\n     47             self._parent._children[self] = None\n---> 48         threading.Thread.start(self)\n        self = <DummyProcess(Thread-16, initial daemon)>\n     49 \n     50     @property\n     51     def exitcode(self):\n     52         if self._start_called and not self.is_alive():\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/threading.py in start(self=<DummyProcess(Thread-16, initial daemon)>)\n    841         if self._started.is_set():\n    842             raise RuntimeError(\"threads can only be started once\")\n    843         with _active_limbo_lock:\n    844             _limbo[self] = self\n    845         try:\n--> 846             _start_new_thread(self._bootstrap, ())\n        self._bootstrap = <bound method Thread._bootstrap of <DummyProcess(Thread-16, initial daemon)>>\n    847         except Exception:\n    848             with _active_limbo_lock:\n    849                 del _limbo[self]\n    850             raise\n\nRuntimeError: can't start new thread\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nRuntimeError                                       Mon Apr 12 15:38:22 2021\nPID: 2258Python 3.6.3: /apps/applications/python/anaconda3/5.0.1/bin/python\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]]), array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), {'f1_0': make_scorer(f1_score, pos_label=0), 'f1_1': make_scorer(f1_score, pos_label=1), 'prec_0': make_scorer(precision_score, pos_label=0), 'prec_1': make_scorer(precision_score, pos_label=1), 'rec_0': make_scorer(recall_score, pos_label=0), 'rec_1': make_scorer(recall_score, pos_label=1), 'roc': make_scorer(roc_auc_score)}, array([    0,     1,     2, ..., 29654, 29655, 29656]), array([   13,    14,    15, ..., 29594, 29604, 29608]), 3, {'bootstrap': False, 'class_weight': {0: 1, 1: 9}, 'criterion': 'gini', 'max_depth': 100, 'max_features': 25, 'min_samples_leaf': 50, 'min_samples_split': 180, 'n_estimators': 200}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]]), array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), {'f1_0': make_scorer(f1_score, pos_label=0), 'f1_1': make_scorer(f1_score, pos_label=1), 'prec_0': make_scorer(precision_score, pos_label=0), 'prec_1': make_scorer(precision_score, pos_label=1), 'rec_0': make_scorer(recall_score, pos_label=0), 'rec_1': make_scorer(recall_score, pos_label=1), 'roc': make_scorer(roc_auc_score)}, array([    0,     1,     2, ..., 29654, 29655, 29656]), array([   13,    14,    15, ..., 29594, 29604, 29608]), 3, {'bootstrap': False, 'class_weight': {0: 1, 1: 9}, 'criterion': 'gini', 'max_depth': 100, 'max_features': 25, 'min_samples_leaf': 50, 'min_samples_split': 180, 'n_estimators': 200})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), X=memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]]), y=array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), scorer={'f1_0': make_scorer(f1_score, pos_label=0), 'f1_1': make_scorer(f1_score, pos_label=1), 'prec_0': make_scorer(precision_score, pos_label=0), 'prec_1': make_scorer(precision_score, pos_label=1), 'rec_0': make_scorer(recall_score, pos_label=0), 'rec_1': make_scorer(recall_score, pos_label=1), 'roc': make_scorer(roc_auc_score)}, train=array([    0,     1,     2, ..., 29654, 29655, 29656]), test=array([   13,    14,    15, ..., 29594, 29604, 29608]), verbose=3, parameters={'bootstrap': False, 'class_weight': {0: 1, 1: 9}, 'criterion': 'gini', 'max_depth': 100, 'max_features': 25, 'min_samples_leaf': 50, 'min_samples_split': 180, 'n_estimators': 200}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas... random_state=None, verbose=0, warm_start=False)>\n        X_train = memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]])\n        y_train = array([ 1.,  1.,  1., ...,  0.,  0.,  0.])\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/ensemble/forest.py in fit(self=RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), X=array([[ 0.65200001,  0.79699999,  0.72799999, ....        0.74536127,  0.90534204]], dtype=float32), y=array([[ 1.],\n       [ 1.],\n       [ 1.],\n       ..., \n       [ 0.],\n       [ 0.],\n       [ 0.]]), sample_weight=array([ 9.,  9.,  9., ...,  1.,  1.,  1.]))\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 199\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    744             raise ValueError('This Parallel instance is already running')\n    745         # A flag used to abort the dispatching of jobs in case an\n    746         # exception is found\n    747         self._aborting = False\n    748         if not self._managed_backend:\n--> 749             n_jobs = self._initialize_backend()\n        n_jobs = undefined\n        self._initialize_backend = <bound method Parallel._initialize_backend of Parallel(n_jobs=-1)>\n    750         else:\n    751             n_jobs = self._effective_n_jobs()\n    752 \n    753         iterator = iter(iterable)\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _initialize_backend(self=Parallel(n_jobs=-1))\n    542 \n    543     def _initialize_backend(self):\n    544         \"\"\"Build a process or thread pool and return the number of workers\"\"\"\n    545         try:\n    546             n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n--> 547                                              **self._backend_args)\n        self._backend_args = {'context': <multiprocessing.context.ForkContext object>, 'max_nbytes': 1048576, 'mmap_mode': 'r', 'temp_folder': None, 'verbose': 0}\n    548             if self.timeout is not None and not self._backend.supports_timeout:\n    549                 warnings.warn(\n    550                     'The backend class {!r} does not support timeout. '\n    551                     \"You have set 'timeout={}' in Parallel but \"\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in configure(self=<sklearn.externals.joblib._parallel_backends.ThreadingBackend object>, n_jobs=32, parallel=Parallel(n_jobs=-1), **backend_args={'context': <multiprocessing.context.ForkContext object>, 'max_nbytes': 1048576, 'mmap_mode': 'r', 'temp_folder': None, 'verbose': 0})\n    245         n_jobs = self.effective_n_jobs(n_jobs)\n    246         if n_jobs == 1:\n    247             # Avoid unnecessary overhead and use sequential backend instead.\n    248             raise FallbackToBackend(SequentialBackend())\n    249         self.parallel = parallel\n--> 250         self._pool = ThreadPool(n_jobs)\n        self._pool = undefined\n        n_jobs = 32\n    251         return n_jobs\n    252 \n    253 \n    254 class MultiprocessingBackend(PoolManagerMixin, AutoBatchingMixin,\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py in __init__(self=<multiprocessing.pool.ThreadPool object>, processes=32, initializer=None, initargs=())\n    784     def Process(*args, **kwds):\n    785         from .dummy import Process\n    786         return Process(*args, **kwds)\n    787 \n    788     def __init__(self, processes=None, initializer=None, initargs=()):\n--> 789         Pool.__init__(self, processes, initializer, initargs)\n        self = <multiprocessing.pool.ThreadPool object>\n        processes = 32\n        initializer = None\n        initargs = ()\n    790 \n    791     def _setup_queues(self):\n    792         self._inqueue = queue.Queue()\n    793         self._outqueue = queue.Queue()\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py in __init__(self=<multiprocessing.pool.ThreadPool object>, processes=32, initializer=None, initargs=(), maxtasksperchild=None, context=None)\n    169         if initializer is not None and not callable(initializer):\n    170             raise TypeError('initializer must be a callable')\n    171 \n    172         self._processes = processes\n    173         self._pool = []\n--> 174         self._repopulate_pool()\n        self._repopulate_pool = <bound method Pool._repopulate_pool of <multiprocessing.pool.ThreadPool object>>\n    175 \n    176         self._worker_handler = threading.Thread(\n    177             target=Pool._handle_workers,\n    178             args=(self, )\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py in _repopulate_pool(self=<multiprocessing.pool.ThreadPool object>)\n    234                                    self._wrap_exception)\n    235                             )\n    236             self._pool.append(w)\n    237             w.name = w.name.replace('Process', 'PoolWorker')\n    238             w.daemon = True\n--> 239             w.start()\n        w.start = <bound method DummyProcess.start of <DummyProcess(Thread-16, initial daemon)>>\n    240             util.debug('added worker')\n    241 \n    242     def _maintain_pool(self):\n    243         \"\"\"Clean up any exited workers and start replacements for them.\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/dummy/__init__.py in start(self=<DummyProcess(Thread-16, initial daemon)>)\n     43     def start(self):\n     44         assert self._parent is current_process()\n     45         self._start_called = True\n     46         if hasattr(self._parent, '_children'):\n     47             self._parent._children[self] = None\n---> 48         threading.Thread.start(self)\n        self = <DummyProcess(Thread-16, initial daemon)>\n     49 \n     50     @property\n     51     def exitcode(self):\n     52         if self._start_called and not self.is_alive():\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/threading.py in start(self=<DummyProcess(Thread-16, initial daemon)>)\n    841         if self._started.is_set():\n    842             raise RuntimeError(\"threads can only be started once\")\n    843         with _active_limbo_lock:\n    844             _limbo[self] = self\n    845         try:\n--> 846             _start_new_thread(self._bootstrap, ())\n        self._bootstrap = <bound method Thread._bootstrap of <DummyProcess(Thread-16, initial daemon)>>\n    847         except Exception:\n    848             with _active_limbo_lock:\n    849                 del _limbo[self]\n    850             raise\n\nRuntimeError: can't start new thread\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mJoblibRuntimeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-ab11823add8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\ncv = \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mbest_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameter_tune\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlots_of_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroupskfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdf_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_scores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-fa568b5055a5>\u001b[0m in \u001b[0;36mhyperparameter_tune\u001b[0;34m(base_model, parameters, kfold, X, y, groups)\u001b[0m\n\u001b[1;32m     25\u001b[0m                                       \u001b[0;31m#random_state=SEED)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0moptimal_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mstop_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 639\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mJoblibRuntimeError\u001b[0m: JoblibRuntimeError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/runpy.py in _run_code(code=<code object <module> at 0x7fb2d2d6a1e0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/apps/applications/python/anaconda3/5.0.1/lib/py...ges/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/apps/applic.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x7fb2d2d6a1e0, file \"/...3.6/site-packages/ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': '/apps/applications/python/anaconda3/5.0.1/lib/py...ges/__pycache__/ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': '/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...b/python3.6/site-packages/ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from '/apps/applic.../python3.6/site-packages/ipykernel/kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/traitlets/config/application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel/kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    472             return self.subapp.start()\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         try:\n--> 477             ioloop.IOLoop.instance().start()\n    478         except KeyboardInterrupt:\n    479             pass\n    480 \n    481 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/zmq/eventloop/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/tornado/ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/tornado/stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel/kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': 'lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2021, 4, 12, 12, 38, 21, 586289, tzinfo=tzutc()), 'msg_id': '6A4618395CCC4412971D89EF39B98536', 'msg_type': 'execute_request', 'session': 'C5862273C9D244BCA09FCFD7ABDD778B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '6A4618395CCC4412971D89EF39B98536', 'msg_type': 'execute_request', 'parent_header': {}})\n    230             self.log.warn(\"Unknown message type: %r\", msg_type)\n    231         else:\n    232             self.log.debug(\"%s: %s\", msg_type, msg)\n    233             self.pre_handler_hook()\n    234             try:\n--> 235                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'C5862273C9D244BCA09FCFD7ABDD778B']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': 'lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2021, 4, 12, 12, 38, 21, 586289, tzinfo=tzutc()), 'msg_id': '6A4618395CCC4412971D89EF39B98536', 'msg_type': 'execute_request', 'session': 'C5862273C9D244BCA09FCFD7ABDD778B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '6A4618395CCC4412971D89EF39B98536', 'msg_type': 'execute_request', 'parent_header': {}}\n    236             except Exception:\n    237                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    238             finally:\n    239                 self.post_handler_hook()\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel/kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'C5862273C9D244BCA09FCFD7ABDD778B'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': 'lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2021, 4, 12, 12, 38, 21, 586289, tzinfo=tzutc()), 'msg_id': '6A4618395CCC4412971D89EF39B98536', 'msg_type': 'execute_request', 'session': 'C5862273C9D244BCA09FCFD7ABDD778B', 'username': 'username', 'version': '5.0'}, 'metadata': {}, 'msg_id': '6A4618395CCC4412971D89EF39B98536', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel/ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    191 \n    192         self._forward_input(allow_stdin)\n    193 \n    194         reply_content = {}\n    195         try:\n--> 196             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = 'lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')'\n        store_history = True\n        silent = False\n    197         finally:\n    198             self._restore_input()\n    199 \n    200         if res.error_before_exec is not None:\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/ipykernel/zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')',), **kwargs={'silent': False, 'store_history': True})\n    528             )\n    529         self.payload_manager.write_payload(payload)\n    530 \n    531     def run_cell(self, *args, **kwargs):\n    532         self._last_traceback = None\n--> 533         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')',)\n        kwargs = {'silent': False, 'store_history': True}\n    534 \n    535     def _showtraceback(self, etype, evalue, stb):\n    536         # try to preserve ordering of tracebacks and print statements\n    537         sys.stdout.flush()\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')', store_history=True, silent=False, shell_futures=True)\n   2693                 self.displayhook.exec_result = result\n   2694 \n   2695                 # Execute the user code\n   2696                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2697                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2698                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2699                 \n   2700                 self.last_execution_succeeded = not has_raised\n   2701 \n   2702                 # Reset this so later displayed values do not modify the\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.For object>], cell_name='<ipython-input-10-ab11823add8f>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 7fb282a16320, executi..._before_exec=None error_in_exec=None result=None>)\n   2797 \n   2798         try:\n   2799             for i, node in enumerate(to_run_exec):\n   2800                 mod = ast.Module([node])\n   2801                 code = compiler(mod, cell_name, \"exec\")\n-> 2802                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x7fb282a18300, file \"<ipython-input-10-ab11823add8f>\", line 25>\n        result = <ExecutionResult object at 7fb282a16320, executi..._before_exec=None error_in_exec=None result=None>\n   2803                     return True\n   2804 \n   2805             for i, node in enumerate(to_run_interactive):\n   2806                 mod = ast.Interactive([node])\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/IPython/core/interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x7fb282a18300, file \"<ipython-input-10-ab11823add8f>\", line 25>, result=<ExecutionResult object at 7fb282a16320, executi..._before_exec=None error_in_exec=None result=None>)\n   2857         outflag = True  # happens in more places, so it's easier as default\n   2858         try:\n   2859             try:\n   2860                 self.hooks.pre_run_code_hook()\n   2861                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2862                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x7fb282a18300, file \"<ipython-input-10-ab11823add8f>\", line 25>\n        self.user_global_ns = {'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'GroupKFold': <class 'sklearn.model_selection._split.GroupKFold'>, 'In': ['', 'import numpy as np\\nimport pandas as pd\\nfrom skle...t listdir, chdir\\nfrom os.path import isfile, join', 'def hyperparameter_tune(base_model, parameters, ...timal_model.best_score_, optimal_model.cv_results', \"mypath = '/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity'\", \"allfiles = [f for f in listdir(mypath) if f.endswith('norm.csv') and f[0].isdigit()]\", 'allfiles', 'chdir(mypath)', 'li = []\\nfor filename in allfiles:\\n    df = pd.re...\\nframe = pd.concat(li, axis=0, ignore_index=True)', \"frame.firedate = pd.to_datetime(frame.firedate).dt.strftime('%Y%m%d')\", \"df_part = frame[['id', 'firedate', 'max_temp', '...{0:1,1:300},{0:1,1:400},{0:1,1:500},{0:1,1:1000}]\", 'lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')'], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {5: ['2016_norm.csv', '2015_norm.csv', '2014_norm.csv', '2017_norm.csv', '2012_norm.csv', '2011_norm.csv', '2010_norm.csv', '2018_norm.csv', '2013_norm.csv']}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'StandardScaler': <class 'sklearn.preprocessing.data.StandardScaler'>, 'StratifiedKFold': <class 'sklearn.model_selection._split.StratifiedKFold'>, 'X_': array([[ 0.652     ,  0.797     ,  0.728     , ....  0.09141531,\n         0.74536127,  0.90534205]]), ...}\n        self.user_ns = {'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'GroupKFold': <class 'sklearn.model_selection._split.GroupKFold'>, 'In': ['', 'import numpy as np\\nimport pandas as pd\\nfrom skle...t listdir, chdir\\nfrom os.path import isfile, join', 'def hyperparameter_tune(base_model, parameters, ...timal_model.best_score_, optimal_model.cv_results', \"mypath = '/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity'\", \"allfiles = [f for f in listdir(mypath) if f.endswith('norm.csv') and f[0].isdigit()]\", 'allfiles', 'chdir(mypath)', 'li = []\\nfor filename in allfiles:\\n    df = pd.re...\\nframe = pd.concat(li, axis=0, ignore_index=True)', \"frame.firedate = pd.to_datetime(frame.firedate).dt.strftime('%Y%m%d')\", \"df_part = frame[['id', 'firedate', 'max_temp', '...{0:1,1:300},{0:1,1:400},{0:1,1:500},{0:1,1:1000}]\", 'lots_of_parameters = {\\n    \"max_depth\": depth, #...ams\")\\n    df_short.to_csv(\\'rf_random_search.csv\\')'], 'KFold': <class 'sklearn.model_selection._split.KFold'>, 'Out': {5: ['2016_norm.csv', '2015_norm.csv', '2014_norm.csv', '2017_norm.csv', '2012_norm.csv', '2011_norm.csv', '2010_norm.csv', '2018_norm.csv', '2013_norm.csv']}, 'RandomForestClassifier': <class 'sklearn.ensemble.forest.RandomForestClassifier'>, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'StandardScaler': <class 'sklearn.preprocessing.data.StandardScaler'>, 'StratifiedKFold': <class 'sklearn.model_selection._split.StratifiedKFold'>, 'X_': array([[ 0.652     ,  0.797     ,  0.728     , ....  0.09141531,\n         0.74536127,  0.90534205]]), ...}\n   2863             finally:\n   2864                 # Reset our crash handler in place\n   2865                 sys.excepthook = old_excepthook\n   2866         except SystemExit as e:\n\n...........................................................................\n/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/<ipython-input-10-ab11823add8f> in <module>()\n     23 results = pd.DataFrame(columns=columns_sel)\n     24 \n     25 for i in folds:\n     26     print(\"\\ncv = \", i)\n     27     start = time.time()\n---> 28     best_params, best_score, full_scores = hyperparameter_tune(rf, lots_of_parameters, i, X_, y_, groupskfold)\n     29 \n     30     df_results = pd.DataFrame.from_dict(full_scores)\n     31     df_results['folds'] = int(i)\n     32     #df_results.to_csv('/home/sgirtsou/Documents/GridSearchCV/RF/RFcv_25kbalanced_noshufflestrictcriterion.csv')\n\n...........................................................................\n/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/<ipython-input-2-fa568b5055a5> in hyperparameter_tune(base_model=RandomForestClassifier(bootstrap=True, class_wei...te=None, verbose=0,\n            warm_start=False), parameters={'bootstrap': [True, False], 'class_weight': [{0: 1, 1: 9}, {0: 1, 1: 300}, {0: 1, 1: 400}, {0: 1, 1: 500}, {0: 1, 1: 1000}], 'criterion': ['gini', 'entropy'], 'max_depth': [10, 20, 100, 200, 400, 500, 700, 1000, 1200, 2000, None], 'max_features': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ...], 'min_samples_leaf': [1, 10, 30, 40, 50, 100, 120, 150], 'min_samples_split': [2, 10, 50, 70, 100, 120, 150, 180, 200, 250, 400, 600, 1000, 1300, 2000], 'n_estimators': [50, 100, 120, 150, 170, 200, 250, 350, 500, 750, 1000, 1400, 1500]}, kfold=10, X=array([[ 0.652     ,  0.797     ,  0.728     , ....  0.09141531,\n         0.74536127,  0.90534205]]), y=array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), groups=array(['20160822', '20160822', '20160721', ..., ...8', '20130805',\n       '20130727'], dtype=object))\n     22                                       refit='rec_1',\n     23                                       verbose=3,\n     24                                       return_train_score=True)\n     25                                       #random_state=SEED)\n     26 \n---> 27     optimal_model.fit(X, y, groups)\n     28 \n     29     stop_time = time.time()\n     30     #scores = cross_validate(optimal_model, X, y, cv=k, scoring= scoring_st, return_train_score=True, return_estimator=True)\n     31     print(\"Elapsed Time:\", time.strftime(\"%H:%M:%S\", time.gmtime(stop_time - start_time)))\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/model_selection/_search.py in fit(self=RandomizedSearchCV(cv=GroupKFold(n_splits=10), e...rer(f1_score, pos_label=0)},\n          verbose=3), X=array([[ 0.652     ,  0.797     ,  0.728     , ....  0.09141531,\n         0.74536127,  0.90534205]]), y=array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), groups=array(['20160822', '20160822', '20160721', ..., ...8', '20130805',\n       '20130727'], dtype=object), **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method _BaseKFold.split of GroupKFold(n_splits=10)>\n        X = array([[ 0.652     ,  0.797     ,  0.728     , ....  0.09141531,\n         0.74536127,  0.90534205]])\n        y = array([ 1.,  1.,  1., ...,  0.,  0.,  0.])\n        groups = array(['20160822', '20160822', '20160721', ..., ...8', '20130805',\n       '20130727'], dtype=object)\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=10), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=10)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nRuntimeError                                       Mon Apr 12 15:38:22 2021\nPID: 2258Python 3.6.3: /apps/applications/python/anaconda3/5.0.1/bin/python\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]]), array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), {'f1_0': make_scorer(f1_score, pos_label=0), 'f1_1': make_scorer(f1_score, pos_label=1), 'prec_0': make_scorer(precision_score, pos_label=0), 'prec_1': make_scorer(precision_score, pos_label=1), 'rec_0': make_scorer(recall_score, pos_label=0), 'rec_1': make_scorer(recall_score, pos_label=1), 'roc': make_scorer(roc_auc_score)}, array([    0,     1,     2, ..., 29654, 29655, 29656]), array([   13,    14,    15, ..., 29594, 29604, 29608]), 3, {'bootstrap': False, 'class_weight': {0: 1, 1: 9}, 'criterion': 'gini', 'max_depth': 100, 'max_features': 25, 'min_samples_leaf': 50, 'min_samples_split': 180, 'n_estimators': 200}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]]), array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), {'f1_0': make_scorer(f1_score, pos_label=0), 'f1_1': make_scorer(f1_score, pos_label=1), 'prec_0': make_scorer(precision_score, pos_label=0), 'prec_1': make_scorer(precision_score, pos_label=1), 'rec_0': make_scorer(recall_score, pos_label=0), 'rec_1': make_scorer(recall_score, pos_label=1), 'roc': make_scorer(roc_auc_score)}, array([    0,     1,     2, ..., 29654, 29655, 29656]), array([   13,    14,    15, ..., 29594, 29604, 29608]), 3, {'bootstrap': False, 'class_weight': {0: 1, 1: 9}, 'criterion': 'gini', 'max_depth': 100, 'max_features': 25, 'min_samples_leaf': 50, 'min_samples_split': 180, 'n_estimators': 200})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/model_selection/_validation.py in _fit_and_score(estimator=RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), X=memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]]), y=array([ 1.,  1.,  1., ...,  0.,  0.,  0.]), scorer={'f1_0': make_scorer(f1_score, pos_label=0), 'f1_1': make_scorer(f1_score, pos_label=1), 'prec_0': make_scorer(precision_score, pos_label=0), 'prec_1': make_scorer(precision_score, pos_label=1), 'rec_0': make_scorer(recall_score, pos_label=0), 'rec_1': make_scorer(recall_score, pos_label=1), 'roc': make_scorer(roc_auc_score)}, train=array([    0,     1,     2, ..., 29654, 29655, 29656]), test=array([   13,    14,    15, ..., 29594, 29604, 29608]), verbose=3, parameters={'bootstrap': False, 'class_weight': {0: 1, 1: 9}, 'criterion': 'gini', 'max_depth': 100, 'max_features': 25, 'min_samples_leaf': 50, 'min_samples_split': 180, 'n_estimators': 200}, fit_params={}, return_train_score=True, return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    453 \n    454     try:\n    455         if y_train is None:\n    456             estimator.fit(X_train, **fit_params)\n    457         else:\n--> 458             estimator.fit(X_train, y_train, **fit_params)\n        estimator.fit = <bound method BaseForest.fit of RandomForestClas... random_state=None, verbose=0, warm_start=False)>\n        X_train = memmap([[ 0.652     ,  0.797     ,  0.728     , ... 0.09141531,\n          0.74536127,  0.90534205]])\n        y_train = array([ 1.,  1.,  1., ...,  0.,  0.,  0.])\n        fit_params = {}\n    459 \n    460     except Exception as e:\n    461         # Note fit time as time until error\n    462         fit_time = time.time() - start_time\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/ensemble/forest.py in fit(self=RandomForestClassifier(bootstrap=False, class_we...  random_state=None, verbose=0, warm_start=False), X=array([[ 0.65200001,  0.79699999,  0.72799999, ....        0.74536127,  0.90534204]], dtype=float32), y=array([[ 1.],\n       [ 1.],\n       [ 1.],\n       ..., \n       [ 0.],\n       [ 0.],\n       [ 0.]]), sample_weight=array([ 9.,  9.,  9., ...,  1.,  1.,  1.]))\n    323             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n    324                              backend=\"threading\")(\n    325                 delayed(_parallel_build_trees)(\n    326                     t, self, X, y, sample_weight, i, len(trees),\n    327                     verbose=self.verbose, class_weight=self.class_weight)\n--> 328                 for i, t in enumerate(trees))\n        i = 199\n    329 \n    330             # Collect newly grown trees\n    331             self.estimators_.extend(trees)\n    332 \n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseForest.fit.<locals>.<genexpr>>)\n    744             raise ValueError('This Parallel instance is already running')\n    745         # A flag used to abort the dispatching of jobs in case an\n    746         # exception is found\n    747         self._aborting = False\n    748         if not self._managed_backend:\n--> 749             n_jobs = self._initialize_backend()\n        n_jobs = undefined\n        self._initialize_backend = <bound method Parallel._initialize_backend of Parallel(n_jobs=-1)>\n    750         else:\n    751             n_jobs = self._effective_n_jobs()\n    752 \n    753         iterator = iter(iterable)\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py in _initialize_backend(self=Parallel(n_jobs=-1))\n    542 \n    543     def _initialize_backend(self):\n    544         \"\"\"Build a process or thread pool and return the number of workers\"\"\"\n    545         try:\n    546             n_jobs = self._backend.configure(n_jobs=self.n_jobs, parallel=self,\n--> 547                                              **self._backend_args)\n        self._backend_args = {'context': <multiprocessing.context.ForkContext object>, 'max_nbytes': 1048576, 'mmap_mode': 'r', 'temp_folder': None, 'verbose': 0}\n    548             if self.timeout is not None and not self._backend.supports_timeout:\n    549                 warnings.warn(\n    550                     'The backend class {!r} does not support timeout. '\n    551                     \"You have set 'timeout={}' in Parallel but \"\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py in configure(self=<sklearn.externals.joblib._parallel_backends.ThreadingBackend object>, n_jobs=32, parallel=Parallel(n_jobs=-1), **backend_args={'context': <multiprocessing.context.ForkContext object>, 'max_nbytes': 1048576, 'mmap_mode': 'r', 'temp_folder': None, 'verbose': 0})\n    245         n_jobs = self.effective_n_jobs(n_jobs)\n    246         if n_jobs == 1:\n    247             # Avoid unnecessary overhead and use sequential backend instead.\n    248             raise FallbackToBackend(SequentialBackend())\n    249         self.parallel = parallel\n--> 250         self._pool = ThreadPool(n_jobs)\n        self._pool = undefined\n        n_jobs = 32\n    251         return n_jobs\n    252 \n    253 \n    254 class MultiprocessingBackend(PoolManagerMixin, AutoBatchingMixin,\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py in __init__(self=<multiprocessing.pool.ThreadPool object>, processes=32, initializer=None, initargs=())\n    784     def Process(*args, **kwds):\n    785         from .dummy import Process\n    786         return Process(*args, **kwds)\n    787 \n    788     def __init__(self, processes=None, initializer=None, initargs=()):\n--> 789         Pool.__init__(self, processes, initializer, initargs)\n        self = <multiprocessing.pool.ThreadPool object>\n        processes = 32\n        initializer = None\n        initargs = ()\n    790 \n    791     def _setup_queues(self):\n    792         self._inqueue = queue.Queue()\n    793         self._outqueue = queue.Queue()\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py in __init__(self=<multiprocessing.pool.ThreadPool object>, processes=32, initializer=None, initargs=(), maxtasksperchild=None, context=None)\n    169         if initializer is not None and not callable(initializer):\n    170             raise TypeError('initializer must be a callable')\n    171 \n    172         self._processes = processes\n    173         self._pool = []\n--> 174         self._repopulate_pool()\n        self._repopulate_pool = <bound method Pool._repopulate_pool of <multiprocessing.pool.ThreadPool object>>\n    175 \n    176         self._worker_handler = threading.Thread(\n    177             target=Pool._handle_workers,\n    178             args=(self, )\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/pool.py in _repopulate_pool(self=<multiprocessing.pool.ThreadPool object>)\n    234                                    self._wrap_exception)\n    235                             )\n    236             self._pool.append(w)\n    237             w.name = w.name.replace('Process', 'PoolWorker')\n    238             w.daemon = True\n--> 239             w.start()\n        w.start = <bound method DummyProcess.start of <DummyProcess(Thread-16, initial daemon)>>\n    240             util.debug('added worker')\n    241 \n    242     def _maintain_pool(self):\n    243         \"\"\"Clean up any exited workers and start replacements for them.\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/multiprocessing/dummy/__init__.py in start(self=<DummyProcess(Thread-16, initial daemon)>)\n     43     def start(self):\n     44         assert self._parent is current_process()\n     45         self._start_called = True\n     46         if hasattr(self._parent, '_children'):\n     47             self._parent._children[self] = None\n---> 48         threading.Thread.start(self)\n        self = <DummyProcess(Thread-16, initial daemon)>\n     49 \n     50     @property\n     51     def exitcode(self):\n     52         if self._start_called and not self.is_alive():\n\n...........................................................................\n/apps/applications/python/anaconda3/5.0.1/lib/python3.6/threading.py in start(self=<DummyProcess(Thread-16, initial daemon)>)\n    841         if self._started.is_set():\n    842             raise RuntimeError(\"threads can only be started once\")\n    843         with _active_limbo_lock:\n    844             _limbo[self] = self\n    845         try:\n--> 846             _start_new_thread(self._bootstrap, ())\n        self._bootstrap = <bound method Thread._bootstrap of <DummyProcess(Thread-16, initial daemon)>>\n    847         except Exception:\n    848             with _active_limbo_lock:\n    849                 del _limbo[self]\n    850             raise\n\nRuntimeError: can't start new thread\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "lots_of_parameters = {\n",
    "    \"max_depth\": depth, #depth of each tree\n",
    "    \"n_estimators\": n_estimators, #trees of the forest\n",
    "    \"min_samples_split\": min_samples_split,\n",
    "    \"min_samples_leaf\": min_samples_leaf,\n",
    "    \"criterion\": criterion,\n",
    "    \"max_features\": max_features,\n",
    "    \"bootstrap\": bootstrap,\n",
    "    \"class_weight\": class_weights\n",
    "}\n",
    "\n",
    "\n",
    "best_scores = []\n",
    "best_parameters = []\n",
    "full_scores = []\n",
    "folds = [10]#range(2, 8)\n",
    "\n",
    "columns_sel = ['mean_test_acc','std_test_acc', 'mean_train_acc', 'std_train_acc','mean_test_AUC','std_test_AUC', 'mean_train_AUC', 'std_train_AUC',\n",
    "              'mean_test_prec','std_test_prec','mean_train_prec','std_train_prec', 'mean_test_rec','std_test_rec',\n",
    "              'mean_train_rec','std_train_rec', 'mean_test_f_score','std_test_f_score', 'mean_train_f_score','std_train_f_score',\n",
    "               'params','folds']\n",
    "\n",
    "results = pd.DataFrame(columns=columns_sel)\n",
    "\n",
    "for i in folds:\n",
    "    print(\"\\ncv = \", i)\n",
    "    start = time.time()\n",
    "    best_params, best_score, full_scores = hyperparameter_tune(rf, lots_of_parameters, i, X_, y_, groupskfold)\n",
    "\n",
    "    df_results = pd.DataFrame.from_dict(full_scores)\n",
    "    df_results['folds'] = int(i)\n",
    "    #df_results.to_csv('/home/sgirtsou/Documents/GridSearchCV/RF/RFcv_25kbalanced_noshufflestrictcriterion.csv')\n",
    "\n",
    "    df_short.to_csv('rf_random_search.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "frame.to_csv('full_dataset_norm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/work2/pa21/sgirtsou/production/datasets/hard_cosine_similarity/full_dataset_norm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'Unnamed: 0.1', 'id', 'firedate', 'max_temp', 'min_temp',\n",
       "       'mean_temp', 'res_max', 'dom_vel', 'rain_7days', 'dem', 'slope',\n",
       "       'curvature', 'aspect', 'ndvi_new', 'evi', 'lst_day', 'lst_night',\n",
       "       'max_dew_temp', 'mean_dew_temp', 'min_dew_temp', 'fire', 'dir_max_1',\n",
       "       'dir_max_2', 'dir_max_3', 'dir_max_4', 'dir_max_5', 'dir_max_6',\n",
       "       'dir_max_7', 'dir_max_8', 'dom_dir_1', 'dom_dir_2', 'dom_dir_3',\n",
       "       'dom_dir_4', 'dom_dir_5', 'dom_dir_6', 'dom_dir_7', 'dom_dir_8',\n",
       "       'corine_111', 'corine_112', 'corine_121', 'corine_122', 'corine_123',\n",
       "       'corine_124', 'corine_131', 'corine_132', 'corine_133', 'corine_141',\n",
       "       'corine_142', 'corine_211', 'corine_212', 'corine_213', 'corine_221',\n",
       "       'corine_222', 'corine_223', 'corine_231', 'corine_241', 'corine_242',\n",
       "       'corine_243', 'corine_244', 'corine_311', 'corine_312', 'corine_313',\n",
       "       'corine_321', 'corine_322', 'corine_323', 'corine_324', 'corine_331',\n",
       "       'corine_332', 'corine_333', 'corine_334', 'corine_411', 'corine_412',\n",
       "       'corine_421', 'corine_422', 'corine_511', 'corine_512', 'corine_521',\n",
       "       'wkd_0', 'wkd_1', 'wkd_2', 'wkd_3', 'wkd_4', 'wkd_5', 'wkd_6',\n",
       "       'month_5', 'month_6', 'month_7', 'month_8', 'month_9', 'month_4',\n",
       "       'frequency', 'f81', 'x', 'y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
